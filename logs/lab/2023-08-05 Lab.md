# Lab Scratchpad for [[2023-08-05]]
## LJP
### Verifying Ground Truth Dataset
- After yesterday's weird results with the consistently smaller LJP values, I realized that I did it wrong because I was replacing the value of the ljp value when I recalculated it
	- Nope I did it fine because when I reran it, I got the same thing
- Getting the error: "numpy.float64 object is not callable" whenever I run the get_potential function
    - ![](https://remnote-user-data.s3.amazonaws.com/jNM9iqb0PkIXgIJB06VvgEm4LiOfY6IV-4uX2AeIpxxTW3hQZG_cxXsRdbSAohFOCii7DYN2KsyZaYgCniQX5voE-l8oCm7Sb5cCqmOJ6hOZgbHLK3vFU2_RrLRoZvPb.png)
    - Giorgos says that happens when he puts brackets around something that's not a list
    - Running a fresh copy of the original notebook works, so something must be wrong with mine
        - Bruh I renamed the function by doing "ljp = ..." when ljp is the name of the function
- Tried doing pairwise differences by calculating the potential twice for each point ‚Üí very suspicious results
	- ![[Screenshot 2023-08-05 at 4.11.21 PM.png|216]]
		- Why are the means so similar (at least they aren‚Äôt exactly the same)
		- What Giorgos actually meant was calculate the potential twice and find differences between those two (not with the dataset)
			- ![[Pasted image 20230805173203.png|152]]
				- Means the GT is problematic? ü§°
- Maybe the real dataset is the ‚Äúljp_dataset.csv‚Äù with way more cases
	- Running the same experiments on that dataset:
		- Looks better!
			- ![[Pasted image 20230805184430.png|275]]
		- Getting MAPE:
			- For some reason, the mean is infinite
			- OK fixed it, it was because some values for the truth were essentially 0, so added a very small value to the denominator (1e-10) ‚Üí pretty high value of 7.657%
				- Then also dropped very small values for LJP values (-1e-15) and got a much better MAPE ‚Üí 1.7566%
				- But this is for 10,000 particles, Dylan says the ellipsoids should actually have 30,000
					- MAPE ‚Üí 1.4778% (which is even better)
		- Trying to calculate with just pairwise differences
			- 2.13% MAPE, so the dataset is reasonably accurate???
### Black Box Model
- Average loss do be looking really weird
	- ![[Pasted image 20230805215213.png|375]]
	- But Giorgos took the loss for each batch, not the average

### PINN
- Trying to use Giorgos‚Äô all-losses method:
	- Without y limits
		- ![[Pasted image 20230805223313.png|325]]
	- y lim between 0 and 0.1
		- ![[Pasted image 20230805223303.png|325]]

## Optic Glomeruli
### Network Architecture
- Convolutional layers first to extract relevant visual information/characteristics, then recurrent layers
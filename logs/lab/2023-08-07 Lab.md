# Lab Scratchpad for [[2023-08-07]]
## Fibrinogen
- Simulation for central E domain alone is only halfway done ;-; (slightly more than half) → extract later today for sure
	- Bernard also didn’t give the fixed PLA so we won’t even be able to start simulations on that one
- Worked on abstract
- Meeting with Bernard
	- Bernard finally fixed the PLA!!!
	- Generating new topology for single PLA, running nvt works!
	- Why make the PLA thicker? To give it high hydrophobic potential so that we can properly see the effect of the interaction with fibrinogen
- Preliminary results for the MD run
	- RMSD at almost 7 ns 
		- ![[Pasted image 20230807215049.png|425]]
	- Radius of gyration at almost 7 ns
		- ![[Screenshot 2023-08-07 at 9.51.57 PM.png|350]]

## Optic Glomeruli
### Network Architecture
- Convolutional layers + LSTM layers:
	- Convolve over spatial dimensions, ignoring time first?
		- Do everything in the 2 dimensions (spatial), keeping the “images” for each frame associated with the time point
		- Outputs some feature “video” that contains dimension-reduced images for each frame
	- LSTM focuses on time
		- Takes the feature “video” and figures out temporal relationship between the frames, also learning the optic glomeruli activity
		- Many-to-many approach, with the same number input frames as the activity output (want activity of optic glomeruli per frame)
- Diagram:
	- ![[Glomeruli Predictor Architecture.png]]
- Implementing separable convolutions on MNIST dataset
	- Just replaced convolutional layers with `SeparableConv2D` layers, but for some reason, the regular Conv2D has better sparse_categorical_accuracy → maybe the less parameters can’t capture the input as well?
		- Basically that’s what happens if your network already originally had very few parameters → increase size of network
			- But be careful of doing too many convolutions that it’s impossible to do any more (or else you get negative value for a dimension)

### Dataset
- Just realized that the numpy arrays generated using flystim are different lengths (in terms of time) → need to standardize somehow
	- Might have to do with frame rate because the “standard” videos (without altered speed or anything) are 5 seconds long when they should really be 6.5 seconds
		- But even the 5 seconds long videos have different lengths (# of frames)
		- Basically what start_stim does is once it’s called, it appends each frame to a list (stim_frames), until stop_stim is called, which resets everything and clears the stim_list that had contained any loaded stimuli
		- What if we use start_stim twice?
			1. Constant gray background
			2. Load the actual stimulus
			- Still inaccurate
	- Actually, it might be the use of the time.sleep function, perhaps it’s not delaying for the exact number of seconds as it says
		- Never mind that did not fix the problem
## LJP
- Finalized abstract
- Turned the jupyter notebook into a bunch of python scripts to run on an a100 node (running even just 50 epochs took )
- Turns out when Dylan said “# of points” he meant the actual # of points inside the ellipsoid (not just in the unit cube) → need to rerun 60,000 points case again
	- Very good MAPE: 0.96%

### Black Box Model
- Testing the effect of the number of training examples per case → use the training data for 30,000 from cases 1, 2, and 3 for black box model
	- Going to run generation code for each case, then concatenate the dataframes together
- To make things easier, I made a new notebook just for generating this training dataset
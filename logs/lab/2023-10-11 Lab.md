« [[2023-10-10 Lab]] | [[2023-10-12 Lab]] » 
# Lab Scratchpad for [[2023-10-11]]
I have a suspicion that the fit_generator method is not suitable given my generator, so I’m going to take more control over the training by using the train_on_batch method instead. Since train_on_batch doesn’t output the loss and record it, I found a [[Viewing loss and metrics during training using train_on_batch|handy code snippet]] that enables you to keep track of metrics (like training/validation loss) as the model trains.

Before, I did not partition some of the data off into a validation set, so I had no real way of gauging how well the model was doing (besides training loss, which can’t tell me about overfitting/underfitting) besides testing it on the heldout stimulus — obviously, very ineffective. And now that has come back to bite me because of how poorly the model is doing (this was without implementation of “train_on_batch":
![[Pasted image 20231011175358.png|425]]
- Of course, it could be a problem with the way the prediction is done — through the use of “predict_generator”, which also takes in batches, similarly to how fit_generator works.
- Besides the choppiness of the data, it also looks to be *scaled* down by some consistent factor, which could be another error I made in the code
- Poor performance is very strange, though, because after training for only 25 epochs, model.fit_generator reported the loss as ~0.03 — clearly, the actual loss on the held out data is much greater!?

Now I’m trying to play around with different predict functions:

## LJP Meeting
Generating ground truth data:
- Adjust semi-axis lengths to keep volume same so that N can be kept constant
- Generate the ground truth based on ground truth using machine learning
	- Interpolate the missing part?
- If training doesn’t go so well in the beginning, just fix the orientations and vary R

Choosing a loss function:
- Pretty sure what Giorgos is talking about is NOT a loss function?

Action Items:
- [x] Figure out mpi4py on seawulf: [How to use Python on Seawulf | Division of Information Technology](https://it.stonybrook.edu/help/kb/how-to-use-python-on-seawulf) #todo/action 
	- I realized just how incompetent Stonybrook’s seawulf documentation is, so for all other Seawulf issues, just use Princeton’s documentation: [Knowledge Base | Princeton Research Computing](https://researchcomputing.princeton.edu/support/knowledge-base)
- [ ] Get familiar with PINNs: [Physics Informed Neural Networks (PINNs): An Intuitive Guide | by Ian Henderson | Towards Data Science](https://towardsdatascience.com/physics-informed-neural-networks-pinns-an-intuitive-guide-fff138069563)
- [ ] Read this paper: [2110.09813.pdf](https://arxiv.org/pdf/2110.09813.pdf) 

![[Screenshot 2023-10-11 at 7.04.52 PM.png]]
- Actually, there really is no boundary/governing/initial conditions -- no derivatives are part of the equation, so no need for anything besides $L_0$
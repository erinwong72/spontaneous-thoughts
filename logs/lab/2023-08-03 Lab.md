# # Lab Scratchpad for [[2023-08-03]]
## LJP
- Started working on the abstract: apparently it has to be ~1 page long? (not really an abstract at that point)
	- Worked on data section (validating ground truth) and started writing about the neural networks we are implementing/plan to implement
- Shared jupyter notebook tutorial with Dylan
- Editing black-box model:
	- Increasing depth (adding more fully connected layers)
		- Trying to use a gpu on the login node: I think I have to install conda, to enable cuda use → going through all the steps to install tensorflow again
	- Increasing learning rate
	- Implementing dropout
	- Early stopping, just in case
  - Improving PINN
	  - Updated with code that prints out average loss per epoch → remains the same
		  - Changing activation function?
	  - Giorgos just recommended fixing the sigma0 and seeing if the rest was functional—seems ok to me, but the loss oscillates after very few epochs
		  - ![[Pasted image 20230803160442.png|425]]
		  - To visualize everything, added a function to save to dataframe
	  - Decreasing learning rate and running for longer (as well as visualizing average loss) seems to be helping
		  - LR: 0.00007
			  - ![[Pasted image 20230803172515.png|325]]
		  - LR: 0.00008
			  - ![[Pasted image 20230803173019.png|325]]
	  - For 100 epochs
		  - ![[Pasted image 20230803174319.png|350]]
	  - Maybe average is wrong?
	  - WIth He intialization and scheduled LR:
		  - ![[Pasted image 20230803181525.png|350]]
	  - He initialization was the problem—stick with the one Giorgos used
		  - Really good (well at least better than before), for 100 epochs
			  - ![[Pasted image 20230803183655.png|325]]
		  - Adding in sigma0
			  - ![[Pasted image 20230803192211.png|325]]
			  - Need to reduce learning rate when it plateaus
		  - ReduceLROnPleateau to the rescue!
			  - Instead of applying to each batch, apply to average training loss per epoch
			  - ![[Pasted image 20230803195644.png|325]]
		  - 250 epochs, changed hidden layers to have 256 nodes each (wider and shallower preferred over deeper)
			  - ![[Pasted image 20230803211501.png|325]]
			  - Trying sigmoid activation function, claimed to be the best choice for PINNs because it retains similar shape across all levels of differentiation
				  - Seems to be descending, very slowly, compared to ReLU, but a lot more consistent (no sudden spikes)
					  - ![[Pasted image 20230803214106.png|300]]
			  - Tanh: veryyyy slooowww
	  - Might want to try Xavier initialization and L-BFGS optimizer after Adam optimization
	  - Take notes on physical activation function paper tomorrow
	  - Ways to improve PINN: [10 Useful Hints and Tricks for Improving Physics-Informed Neural Networks (PINNs) | by Rafael Bischof | Towards Data Science](https://towardsdatascience.com/10-useful-hints-and-tricks-for-improving-pinns-1a5dd7b86001)
		  - Dynamically changing learning rate

## Fibrinogen
- Helped Matthew run his npt, shared my parameters with the group
- Meeting with Bernard to resolve issues:

## Ethics Outline
- Finished it and wrote part of the abstract
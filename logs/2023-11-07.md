« [[2023-11-06]] | [[2023-11-08]] » 
# Thoughts for the day...
I got a 93 on my physics test 😭, probably because of the last parallel plates problem. I feel like such a failure because I actually studied really hard for it, but how am I so stupid ahhhh 🤦‍♀️, I figured it out easily after but second-guessed myself during the test. Because of that my average dropped by one whole point to a 96.
## LC Responses
I fixed all the stimuli problems:
- Compiled them all into one numpy file with shape (2250, 30, 30, 10) since we’re now taking seconds (1,6) instead of (1,5)
- Renamed them to match stimuli names
- Ensured the order of the stimuli in the large numpy file (processed_X_data.npy) was correct
- Sanity-checked with the generated videos, comparing with previous (distorted) stimuli
- Uploaded to the [shared google drive](https://drive.google.com/drive/folders/1QCppin68UBNTbf_P0y6zO3dgkQ3M8YLZ)

More github shenanigans :(, eventually redownloaded repo and put files, then pushed, would’ve been much easier without git LFS 😡

Re-training on new stimuli, waiting on Dr. Cowley for y responses.

## PINN for LJP
Finally figured out what has been going on with the PINN:
- I had a bit of a hard time getting the PINN to train because (I didn’t know this then) the learning rate was too low -> infinite loss as early as the first batch.
- Training seems to be the best when the learning rate is around 1e-2 for both epsilon and sigma and sigma0

Actually, it seems to be either an input data scaling or weights initialization problem, bc if the first batch is doing so poorly, then learning rate has no effect.

The first batch of predictions is actually so bad ;-;:
- The ground truth (for the first batch) does have 13 outliers though, not sure if those make a big impact
![[Pasted image 20231107141437.png|375]]![[Pasted image 20231107141343.png|375]]
![[Screenshot 2023-11-07 at 2.16.08 PM.png|275]]

Outliers:
![[Screenshot 2023-11-07 at 2.13.53 PM.png|275]] 

Possible solutions to try
- Batch norm?
- Cap ground truth values at ~50

Batch norm doesn’t seem to make much of a difference, but capping GT values does:
But, no matter how extreme I cap the values (this is between -10 and 10), the first batch loss is insanely high → weights initialization problem?
![[Screenshot 2023-11-07 at 3.30.49 PM.png|249]]
- Try He initialization
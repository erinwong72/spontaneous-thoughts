# Attention gate

Another step/bottleneck mechanism that alters the flow of information. It suppresses and amplifies certain regions (for an image, CNN model) or words, etc. based on relevance, by multiplying a matrix (the gate) to the output before it's fed into the next layer.

It's different from the [[Attention mechanism]] in a transformer because it is used to focus on specific areas of the input rather than draw connections between them or remember them.

https://ai.stackexchange.com/questions/25099/what-is-the-difference-between-attention-gate-and-cnn-filters #toresearch 
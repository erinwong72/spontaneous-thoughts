« [[2023-10-10 Lab]] | [[2023-10-12 Lab]] » 
# Lab Scratchpad for [[2023-10-11]]
I have a suspicion that the fit_generator method is not suitable given my generator, so I’m going to take more control over the training by using the train_on_batch method instead. Since train_on_batch doesn’t output the loss and record it, I found a [[Viewing loss and metrics during training using train_on_batch|handy code snippet]] that enables you to keep track of metrics (like training/validation loss) as the model trains.

Before, I did not partition some of the data off into a validation set, so I had no real way of gauging how well the model was doing (besides training loss, which can’t tell me about overfitting/underfitting) besides testing it on the heldout stimulus — obviously, very ineffective. And now that has come back to bite me because of how poorly the model is doing (this was without implementation of “train_on_batch":
![[Pasted image 20231011175358.png|425]]
- Of course, it could be a problem with the way the prediction is done — through the use of “predict_generator”, which also takes in batches, similarly to how fit_generator works.
- Besides the choppiness of the data, it also looks to be *scaled* down by some consistent factor, which could be another error I made in the code
- Poor performance is very strange, though, because after training for only 25 epochs, model.fit_generator reported the loss as ~0.03 — clearly, the actual loss on the held out data is much greater!?

Now I’m trying to play around with different predict functions:
